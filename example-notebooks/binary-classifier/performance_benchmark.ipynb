{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seismometer Performance Benchmark\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Measures time and memory for **each operation** in the classifier_bin notebook:\n",
    "- **18 operations** measured individually (startup + 17 operations)\n",
    "- **2 configurations**: per_context OFF vs ON\n",
    "- **Multiple data sizes**: base dataset + scaled versions\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Edit `SCALE_SIZES` in cell 2 to control which scaled datasets to generate:\n",
    "```python\n",
    "SCALE_SIZES = [1_000_000, 10_000_000]  # 1M, 10M\n",
    "```\n",
    "\n",
    "The benchmark will run on:\n",
    "- Base dataset (data/predictions.parquet as-is)\n",
    "- Scaled datasets (generated in data/scaled/)\n",
    "\n",
    "## Output\n",
    "\n",
    "4 tables showing:\n",
    "1. **Memory** - Config A (baseline) with % of total\n",
    "2. **Memory** - Config B (per_context) with overhead %\n",
    "3. **Time** - Config A (baseline) with % of total\n",
    "4. **Time** - Config B (per_context) with overhead %\n",
    "\n",
    "**Just run all cells!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports loaded\n",
      "✓ Configuration: Base + 2 scaled sizes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import gc\n",
    "import threading\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent / 'src'))\n",
    "import seismometer as sm\n",
    "\n",
    "print(\"✓ Imports loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Edit this list to change benchmark data sizes\n",
    "# ============================================================================\n",
    "SCALE_SIZES = [1_000_000, 2_000_000]  # 1M, 10M rows\n",
    "# Examples:\n",
    "#   [1_000_000]                           # Just 1M (base + 1M = 2 sizes)\n",
    "#   [1_000_000, 10_000_000]               # 1M, 10M (base + 2 = 3 sizes)\n",
    "#   [5_000_000, 10_000_000, 100_000_000]  # 5M, 10M, 100M (base + 3 = 4 sizes)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"✓ Configuration: Base + {len(SCALE_SIZES)} scaled sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Scaled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scaled datasets: ['2m']...\n",
      "  Base dataset: 104,307 rows\n",
      "  Generating 2m (2,000,000 rows)... ✓\n",
      "✓ Scaled datasets ready\n"
     ]
    }
   ],
   "source": [
    "def create_scaled_datasets(scale_sizes=[1_000_000, 3_000_000]):\n",
    "    \"\"\"Generate scaled datasets from base data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scale_sizes : list[int]\n",
    "        Target row counts for scaled datasets (e.g., [1_000_000, 10_000_000])\n",
    "    \n",
    "    The base dataset (data/predictions.parquet) is used as-is.\n",
    "    Scaled versions are generated in data/scaled/ directory.\n",
    "    \"\"\"\n",
    "    scaled_dir = Path('data/scaled')\n",
    "    scaled_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Dynamically create size_to_suffix mapping\n",
    "    def size_to_label(size):\n",
    "        if size >= 1_000_000:\n",
    "            return f\"{size//1_000_000}m\"\n",
    "        elif size >= 1000:\n",
    "            return f\"{size//1000}k\"\n",
    "        else:\n",
    "            return str(size)\n",
    "    \n",
    "    size_to_suffix = {size: size_to_label(size) for size in scale_sizes}\n",
    "    \n",
    "    # Check which scaled datasets already exist\n",
    "    missing = [s for s in scale_sizes if not (scaled_dir / f'predictions_{size_to_suffix[s]}.parquet').exists()]\n",
    "    \n",
    "    if not missing:\n",
    "        print(f\"✓ All scaled datasets exist: {[size_to_suffix[s] for s in scale_sizes]}\")\n",
    "        return size_to_suffix\n",
    "    \n",
    "    print(f\"Creating scaled datasets: {[size_to_suffix[s] for s in missing]}...\")\n",
    "    \n",
    "    # Load base data\n",
    "    predictions = pl.read_parquet('data/predictions.parquet').with_columns([\n",
    "        pl.col('encounter_id').cast(pl.Utf8), pl.col('patient_nbr').cast(pl.Utf8)\n",
    "    ])\n",
    "    events = pl.read_parquet('data/events.parquet').with_columns([\n",
    "        pl.col('encounter_id').cast(pl.Utf8), pl.col('patient_nbr').cast(pl.Utf8)\n",
    "    ])\n",
    "    \n",
    "    base_len = len(predictions)\n",
    "    print(f\"  Base dataset: {base_len:,} rows\")\n",
    "    \n",
    "    def replicate(df, target_rows):\n",
    "        \"\"\"Replicate dataframe to target row count with unique IDs.\"\"\"\n",
    "        replicas = int(np.ceil(target_rows / base_len))\n",
    "        return (\n",
    "            pl.concat([df] * replicas).with_row_index('_row')\n",
    "            .with_columns([\n",
    "                (pl.col('encounter_id') + \"_r\" + (pl.col('_row') // base_len).cast(pl.Utf8)).alias('encounter_id'),\n",
    "                (pl.col('patient_nbr') + \"_r\" + (pl.col('_row') // base_len).cast(pl.Utf8)).alias('patient_nbr')\n",
    "            ]).drop('_row').head(target_rows)\n",
    "        )\n",
    "    \n",
    "    # Generate only missing scaled datasets\n",
    "    for target_size in missing:\n",
    "        suffix = size_to_suffix[target_size]\n",
    "        print(f\"  Generating {suffix} ({target_size:,} rows)...\", end=' ', flush=True)\n",
    "        replicate(predictions, target_size).write_parquet(f'data/scaled/predictions_{suffix}.parquet')\n",
    "        replicate(events, int(len(events) * target_size / base_len)).write_parquet(f'data/scaled/events_{suffix}.parquet')\n",
    "        print(\"✓\")\n",
    "    \n",
    "    print(f\"✓ Scaled datasets ready\")\n",
    "    return size_to_suffix\n",
    "\n",
    "# Generate scaled datasets and get the suffix mapping\n",
    "size_to_suffix = create_scaled_datasets(SCALE_SIZES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Infrastructure ready\n"
     ]
    }
   ],
   "source": [
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.peak_rss = 0\n",
    "        self.measuring = False\n",
    "        self.thread = None\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "    \n",
    "    def start(self):\n",
    "        self.peak_rss = 0\n",
    "        self.measuring = True\n",
    "        self.thread = threading.Thread(target=self._sample, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.measuring = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1.0)\n",
    "        return self.peak_rss\n",
    "    \n",
    "    def _sample(self):\n",
    "        while self.measuring:\n",
    "            self.peak_rss = max(self.peak_rss, self.process.memory_info().rss)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "\n",
    "def get_operations(per_context=False):\n",
    "    \"\"\"Return all operations (after startup).\n",
    "    \n",
    "    Note: show_cohort_summaries is called TWICE - once with (False,False) and once with (True,True)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ('show_info', lambda: sm.show_info(plot_help=True)),\n",
    "        ('ExploreSubgroups', lambda: sm.ExploreSubgroups()),\n",
    "        ('feature_alerts', lambda: sm.feature_alerts()),\n",
    "        ('feature_summary', lambda: sm.feature_summary()),\n",
    "        ('cohort_comparison_report', lambda: sm.cohort_comparison_report()),\n",
    "        ('target_feature_summary', lambda: sm.target_feature_summary()),\n",
    "        ('ExploreModelEvaluation', lambda: sm.ExploreModelEvaluation()),\n",
    "        ('plot_model_evaluation', lambda: sm.plot_model_evaluation(\n",
    "            {}, 'Readmitted within 30 Days', 'Risk30DayReadmission', (0.10, 0.20), per_context=per_context\n",
    "        )),\n",
    "        ('ExploreFairnessAudit', lambda: sm.ExploreFairnessAudit()),\n",
    "        ('show_cohort_summaries_baseline', lambda: sm.show_cohort_summaries(by_target=False, by_score=False)),\n",
    "        ('show_cohort_summaries_per_context', lambda: sm.show_cohort_summaries(by_target=True, by_score=True)),\n",
    "        ('ExploreCohortEvaluation', lambda: sm.ExploreCohortEvaluation()),\n",
    "        ('ExploreAnalyticsTable', lambda: sm.ExploreAnalyticsTable()),\n",
    "        ('ExploreOrdinalMetrics', lambda: sm.ExploreOrdinalMetrics()),\n",
    "        ('ExploreCohortOrdinalMetrics', lambda: sm.ExploreCohortOrdinalMetrics()),\n",
    "        ('ExploreCohortLeadTime', lambda: sm.ExploreCohortLeadTime()),\n",
    "        ('ExploreCohortOutcomeInterventionTimes', lambda: sm.ExploreCohortOutcomeInterventionTimes()),\n",
    "    ]\n",
    "\n",
    "\n",
    "def benchmark_dataset(dataset_size, pred_path, event_path, per_context=False):\n",
    "    \"\"\"Measure all operations individually.\n",
    "    \n",
    "    Modifies config.yml temporarily to point to the specified data files,\n",
    "    then restores the original after benchmarking.\n",
    "    \"\"\"\n",
    "    from IPython.utils.capture import capture_output\n",
    "    \n",
    "    config_name = 'per_context' if per_context else 'baseline'\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{dataset_size} - {config_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Backup original config.yml\n",
    "    shutil.copy('config.yml', 'config.yml.backup')\n",
    "\n",
    "    try:\n",
    "        # Modify config.yml to point to the correct data files\n",
    "        # Config has separate fields: data_dir, prediction_path, event_path\n",
    "        # Extract paths relative to data_dir\n",
    "        pred_relative = pred_path.replace('data/', '', 1) if pred_path.startswith('data/') else pred_path\n",
    "        event_relative = event_path.replace('data/', '', 1) if event_path.startswith('data/') else event_path\n",
    "        \n",
    "        with open('config.yml', 'r') as f:\n",
    "            config_original = f.read()\n",
    "        \n",
    "        # Replace the path fields in config (keeping data_dir as \"data\")\n",
    "        import re\n",
    "        config = re.sub(r'prediction_path:\\s*\"[^\"]*\"', f'prediction_path: \"{pred_relative}\"', config_original)\n",
    "        config = re.sub(r'event_path:\\s*\"[^\"]*\"', f'event_path: \"{event_relative}\"', config)\n",
    "        \n",
    "        with open('config.yml', 'w') as f:\n",
    "            f.write(config)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # 1. Startup\n",
    "        print(\"  run_startup...\", end=' ', flush=True)\n",
    "        sm.Seismogram._instances = {}\n",
    "        gc.collect()\n",
    "\n",
    "        monitor = MemoryMonitor()\n",
    "        monitor.start()\n",
    "        t0 = time.perf_counter()\n",
    "        sm.run_startup(config_path='.', log_level=30, reset=True)\n",
    "        t_startup = time.perf_counter() - t0\n",
    "        rss_startup = monitor.stop()\n",
    "\n",
    "        print(f\"✓ {t_startup:.2f}s, {rss_startup/1024/1024:.0f}MB\")\n",
    "        results.append({'operation': 'run_startup', 'time_sec': t_startup, 'peak_rss_mb': rss_startup/1024/1024})\n",
    "\n",
    "        # 2-18. Other operations\n",
    "        for op_name, op_func in get_operations(per_context):\n",
    "            print(f\"  {op_name}...\", end=' ', flush=True)\n",
    "            gc.collect()\n",
    "\n",
    "            monitor = MemoryMonitor()\n",
    "            monitor.start()\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            try:\n",
    "                # Suppress ALL display output (stdout, stderr, widgets, HTML, etc.)\n",
    "                with capture_output(stdout=False, stderr=False, display=True):\n",
    "                    result = op_func()\n",
    "                    \n",
    "                    # For Explore widgets, trigger the update button click\n",
    "                    if op_name.startswith('Explore') and result is not None:\n",
    "                        # Try to find and click the update button\n",
    "                        if hasattr(result, 'children'):\n",
    "                            # Look for button in widget tree\n",
    "                            def find_and_click_update(widget):\n",
    "                                if hasattr(widget, 'description') and 'update' in str(widget.description).lower():\n",
    "                                    # Trigger the button click\n",
    "                                    if hasattr(widget, 'click'):\n",
    "                                        widget.click()\n",
    "                                    elif hasattr(widget, '_click_handlers'):\n",
    "                                        for handler in widget._click_handlers.callbacks:\n",
    "                                            handler(widget)\n",
    "                                    return True\n",
    "                                if hasattr(widget, 'children'):\n",
    "                                    for child in widget.children:\n",
    "                                        if find_and_click_update(child):\n",
    "                                            return True\n",
    "                                return False\n",
    "                            \n",
    "                            find_and_click_update(result)\n",
    "                \n",
    "                t_op = time.perf_counter() - t0\n",
    "                rss_op = monitor.stop()\n",
    "                print(f\"✓ {t_op:.2f}s, {rss_op/1024/1024:.0f}MB\")\n",
    "                results.append({'operation': op_name, 'time_sec': t_op, 'peak_rss_mb': rss_op/1024/1024})\n",
    "            except Exception as e:\n",
    "                monitor.stop()\n",
    "                print(f\"✗ {str(e)[:50]}\")\n",
    "                results.append({'operation': op_name, 'time_sec': None, 'peak_rss_mb': None})\n",
    "\n",
    "        return results\n",
    "\n",
    "    finally:\n",
    "        # Restore original config.yml\n",
    "        shutil.move('config.yml.backup', 'config.yml')\n",
    "        sm.Seismogram._instances = {}\n",
    "        gc.collect()\n",
    "\n",
    "print(\"✓ Infrastructure ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BENCHMARKS: 18 ops × 3 sizes × 2 configs = 108 measurements\n",
      "======================================================================\n",
      "Datasets: 100K, 1M, 2M\n",
      "\n",
      "\n",
      "======================================================================\n",
      "100K - baseline\n",
      "======================================================================\n",
      "  run_startup... ✓ 2.27s, 2742MB\n",
      "  show_info... ✓ 0.01s, 2646MB\n",
      "  ExploreSubgroups... ✓ 0.21s, 2648MB\n",
      "  feature_alerts... ✓ 0.11s, 2699MB\n",
      "  feature_summary... ✓ 0.09s, 2699MB\n",
      "  cohort_comparison_report... ✓ 0.20s, 2739MB\n",
      "  target_feature_summary... ✓ 0.10s, 2739MB\n",
      "  ExploreModelEvaluation... ✓ 3.04s, 2847MB\n",
      "  plot_model_evaluation... ✓ 1.39s, 2856MB\n",
      "  ExploreFairnessAudit... ✓ 0.98s, 2859MB\n",
      "  show_cohort_summaries_baseline... ✓ 0.43s, 2877MB\n",
      "  show_cohort_summaries_per_context... ✓ 3.09s, 2906MB\n",
      "  ExploreCohortEvaluation... ✓ 1.28s, 2906MB\n",
      "  ExploreAnalyticsTable... ✓ 0.65s, 2903MB\n",
      "  ExploreOrdinalMetrics... ✓ 0.95s, 2903MB\n",
      "  ExploreCohortOrdinalMetrics... ✓ 0.92s, 2910MB\n",
      "  ExploreCohortLeadTime... ✓ 0.54s, 2911MB\n",
      "  ExploreCohortOutcomeInterventionTimes... ✓ 3.29s, 2895MB\n",
      "\n",
      "======================================================================\n",
      "100K - per_context\n",
      "======================================================================\n",
      "  run_startup... ✓ 1.80s, 3112MB\n",
      "  show_info... ✓ 0.01s, 3053MB\n",
      "  ExploreSubgroups... ✓ 0.20s, 3052MB\n",
      "  feature_alerts... ✓ 0.12s, 3081MB\n",
      "  feature_summary... ✓ 0.07s, 3081MB\n",
      "  cohort_comparison_report... ✓ 0.16s, 3091MB\n",
      "  target_feature_summary... ✓ 0.09s, 3091MB\n",
      "  ExploreModelEvaluation... ✓ 2.97s, 3115MB\n",
      "  plot_model_evaluation... ✓ 1.42s, 3135MB\n",
      "  ExploreFairnessAudit... ✓ 1.00s, 3135MB\n",
      "  show_cohort_summaries_baseline... ✓ 0.41s, 3138MB\n",
      "  show_cohort_summaries_per_context... ✓ 3.15s, 3157MB\n",
      "  ExploreCohortEvaluation... ✓ 1.29s, 3157MB\n",
      "  ExploreAnalyticsTable... ✓ 0.64s, 3156MB\n",
      "  ExploreOrdinalMetrics... ✓ 0.99s, 3156MB\n",
      "  ExploreCohortOrdinalMetrics... ✓ 0.93s, 3157MB\n",
      "  ExploreCohortLeadTime... ✓ 0.49s, 3157MB\n",
      "  ExploreCohortOutcomeInterventionTimes... ✓ 3.29s, 3160MB\n",
      "\n",
      "======================================================================\n",
      "1M - baseline\n",
      "======================================================================\n",
      "  run_startup... ✓ 19.35s, 5557MB\n",
      "  show_info... ✓ 0.01s, 4263MB\n",
      "  ExploreSubgroups... ✓ 0.40s, 4329MB\n",
      "  feature_alerts... ✓ 0.77s, 4845MB\n",
      "  feature_summary... ✓ 0.58s, 4958MB\n",
      "  cohort_comparison_report... ✓ 0.17s, 4514MB\n",
      "  target_feature_summary... ✓ 0.69s, 4998MB\n",
      "  ExploreModelEvaluation... ✓ 7.62s, 5107MB\n",
      "  plot_model_evaluation... "
     ]
    }
   ],
   "source": [
    "# Auto-detect base dataset size\n",
    "base_pred_path = Path('data/predictions.parquet')\n",
    "base_rows = len(pl.read_parquet(base_pred_path))\n",
    "\n",
    "def format_size(n):\n",
    "    \"\"\"Format row count as human-readable label with rounding.\"\"\"\n",
    "    if n >= 1_000_000:\n",
    "        # Round to nearest 100K for millions\n",
    "        rounded = round(n / 100_000) * 100_000\n",
    "        return f\"{rounded//1_000_000}M\" if rounded >= 1_000_000 else f\"{rounded//1000}K\"\n",
    "    elif n >= 100_000:\n",
    "        # Round to nearest 100K for 100K+\n",
    "        rounded = round(n / 100_000) * 100_000\n",
    "        return f\"{rounded//1000}K\"\n",
    "    elif n >= 1000:\n",
    "        # Round to nearest 10K for smaller sizes\n",
    "        rounded = round(n / 10_000) * 10_000\n",
    "        return f\"{rounded//1000}K\"\n",
    "    else:\n",
    "        return str(n)\n",
    "\n",
    "# Build dataset list: base + scaled\n",
    "datasets = [\n",
    "    (format_size(base_rows), 'data/predictions.parquet', 'data/events.parquet'),\n",
    "]\n",
    "\n",
    "# Add scaled datasets using the size_to_suffix mapping from cell 4\n",
    "for scale_size in SCALE_SIZES:\n",
    "    suffix = size_to_suffix[scale_size]\n",
    "    datasets.append((\n",
    "        format_size(scale_size),\n",
    "        f'data/scaled/predictions_{suffix}.parquet',\n",
    "        f'data/scaled/events_{suffix}.parquet'\n",
    "    ))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"BENCHMARKS: 18 ops × {len(datasets)} sizes × 2 configs = {18*len(datasets)*2} measurements\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Datasets: {', '.join([d[0] for d in datasets])}\")\n",
    "print()\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for dataset_size, pred_path, event_path in datasets:\n",
    "    for per_context in [False, True]:\n",
    "        results = benchmark_dataset(dataset_size, pred_path, event_path, per_context)\n",
    "        for r in results:\n",
    "            r['dataset_size'] = dataset_size\n",
    "            r['config'] = 'per_context' if per_context else 'baseline'\n",
    "            all_results.append(r)\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config A: Baseline\n",
    "mem_a = df[df['config']=='baseline'].pivot_table(index='operation', columns='dataset_size', values='peak_rss_mb', aggfunc='first')\n",
    "\n",
    "# Get column order (sorted by size)\n",
    "cols = sorted(mem_a.columns, key=lambda x: int(x.rstrip('KM')) * (1000 if 'K' in x else 1_000_000 if 'M' in x else 1))\n",
    "mem_a = mem_a[cols]\n",
    "\n",
    "# Add % of total (using largest dataset)\n",
    "largest_col = cols[-1]\n",
    "mem_a['% of Total'] = (mem_a[largest_col] / mem_a[largest_col].max() * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PEAK RSS (MB) - Config A: Baseline (per_context=False)\")\n",
    "print(\"=\"*80)\n",
    "print(mem_a.to_string(float_format=lambda x: f\"{x:,.0f}\"))\n",
    "print(f\"\\nPeak: {' / '.join([f'{col}={mem_a[col].max():.0f}MB' for col in cols])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config B: Per-context\n",
    "mem_b = df[df['config']=='per_context'].pivot_table(index='operation', columns='dataset_size', values='peak_rss_mb', aggfunc='first')\n",
    "\n",
    "# Use same column order as mem_a\n",
    "mem_b = mem_b[cols]\n",
    "\n",
    "# Calculate overhead\n",
    "overhead = []\n",
    "for op in mem_b.index:\n",
    "    if op in mem_a.index:\n",
    "        vals_a = mem_a.loc[op, cols].values\n",
    "        vals_b = mem_b.loc[op, cols].values\n",
    "        avg_oh = ((vals_b - vals_a) / vals_a * 100).mean()\n",
    "        overhead.append(avg_oh)\n",
    "    else:\n",
    "        overhead.append(0)\n",
    "mem_b['vs Baseline'] = [f\"{x:+.0f}%\" if not np.isnan(x) else '-' for x in overhead]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PEAK RSS (MB) - Config B: Per-context (per_context=True)\")\n",
    "print(\"=\"*80)\n",
    "print(mem_b.to_string(float_format=lambda x: f\"{x:,.0f}\"))\n",
    "print(f\"\\nPeak: {' / '.join([f'{col}={mem_b[col].max():.0f}MB' for col in cols])}\")\n",
    "print(f\"Overhead: {(mem_b[largest_col].max()/mem_a[largest_col].max()-1)*100:+.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Time Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config A: Baseline\n",
    "time_a = df[df['config']=='baseline'].pivot_table(index='operation', columns='dataset_size', values='time_sec', aggfunc='first')[cols]\n",
    "\n",
    "# Add % of total (using largest dataset)\n",
    "time_a['% of Total'] = (time_a[largest_col] / time_a[largest_col].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNTIME (seconds) - Config A: Baseline (per_context=False)\")\n",
    "print(\"=\"*80)\n",
    "print(time_a.to_string(float_format=lambda x: f\"{x:,.2f}\"))\n",
    "print(f\"\\nTotal: {' / '.join([f'{col}={time_a[col].sum():.1f}s' for col in cols])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config B: Per-context\n",
    "time_b = df[df['config']=='per_context'].pivot_table(index='operation', columns='dataset_size', values='time_sec', aggfunc='first')[cols]\n",
    "\n",
    "# Calculate overhead\n",
    "time_oh = []\n",
    "for op in time_b.index:\n",
    "    if op in time_a.index:\n",
    "        vals_a = time_a.loc[op, cols].values\n",
    "        vals_b = time_b.loc[op, cols].values\n",
    "        avg_oh = ((vals_b - vals_a) / vals_a * 100).mean()\n",
    "        time_oh.append(avg_oh)\n",
    "    else:\n",
    "        time_oh.append(0)\n",
    "time_b['vs Baseline'] = [f\"{x:+.0f}%\" if not np.isnan(x) else '-' for x in time_oh]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNTIME (seconds) - Config B: Per-context (per_context=True)\")\n",
    "print(\"=\"*80)\n",
    "print(time_b.to_string(float_format=lambda x: f\"{x:,.2f}\"))\n",
    "print(f\"\\nTotal: {' / '.join([f'{col}={time_b[col].sum():.1f}s' for col in cols])}\")\n",
    "print(f\"Overhead: {(time_b[largest_col].sum()/time_a[largest_col].sum()-1)*100:+.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "df.to_csv(f\"benchmark_results_{timestamp}.csv\", index=False)\n",
    "print(f\"\\n✓ Saved: benchmark_results_{timestamp}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
